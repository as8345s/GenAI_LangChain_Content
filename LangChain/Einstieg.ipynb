{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966a7f4f-b0e6-4d49-933f-36ff5258d988",
   "metadata": {},
   "source": [
    "<h1>LangChain Beispiel</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b556eb7-28ee-4a8c-a1f9-69959c1234f6",
   "metadata": {},
   "source": [
    "Wieso sollte man LangChain nutzen und nicht einfach ChatGPT? <br>\n",
    "Oft will man ein eigenes LLM Model nutzen das mit den eigenen bestehenden Daten trainiert wurde. Das hat den Vorteil das keine sensitiven Daten nach außen dringen zu Webseiten, die ein Model bereitstellen und potenziell diese Daten die geschickt wurden speichert. Mit einem eigenen Model bleibt das Model Lokal auf dem PC oder im Unternehmensnetzwerk. Zudem haben Modelle die ChatGPT keinen direkten Zugriff auf unsere Daten.\n",
    "\n",
    "Open-Source Modelle können auch einfach importiert und gespeichert werden, damit fällt das Haupttraining des Netzes weg (ggf. Fine-Tuning). Mit LangChain können solche Applikationen mit Modellen erstellt werden. Das Framework bietet verschiedenste Tools für den Umgang mit solchen Modellen.\n",
    "\n",
    "Als Einstieg wollen wir ein Open-Source Modell nutzen und einfache Textabfragen abwickeln. Weitere Beispiele Folgen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa157755-c197-4c87-8988-4890abcb5adf",
   "metadata": {},
   "source": [
    "<i>Abb1</i>: ChatGPT nutzung. Kommunikation über Internet.\n",
    "\n",
    "<img src=\"./data/img/1_lg.PNG\" width=700 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b191e81-9056-4345-a739-8b62418f4a6e",
   "metadata": {},
   "source": [
    "*Um ChatGPT zu nutzen, muss ein OpenAI API Key bereitgestellt werden. Siehe OpenAI API <br>\n",
    "*Kosten können anfallen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15aa1f-5477-4427-b753-56682ff633cf",
   "metadata": {},
   "source": [
    "Mehr über LangChain:\n",
    "\n",
    "> https://python.langchain.com/v0.2/docs/introduction/ [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "HuggingFace Modelle und Beispiele: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f99f-ea55-4112-93a6-f8667dcadfc1",
   "metadata": {},
   "source": [
    "Es gibt eine große Anzahl von Modellen, die wir nutzen können. Als Einstieg nutzen wir das GPT2 Model.<br>\n",
    "Auf der Webseite von HuggingFace wo die Modelle aufgelistet sind, befinden sich für jedes Model Beschreibungen und Beispiele.\n",
    "\n",
    "Es sollte genug Speicher für ein Model bereitgestellt werden. \n",
    "\n",
    "GPT2: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "Dieses Model wurde mit den Daten aus 8 Millionen Webseiten trainiert. <u>Das Ziel</u>: die nächsten Wörter vorhersagen.\n",
    "\n",
    "Jedes Model kann andere Ziele haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7242cca5-e684-465a-8a5c-223d906e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fccbf0-1397-4fc2-b14e-c3bb17da4224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Model.\n",
    "model       = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Model, welches wir nutzen wollen. \n",
    "tokenizer   = AutoTokenizer.from_pretrained(\"gpt2\")  # Tokenizer des Models => Bereite Eingabe vor. \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98b80b-af3b-4045-97bc-32b5280e6c9b",
   "metadata": {},
   "source": [
    "Jedes Model ist anders und braucht einen anderen Input. Je nach Model und Aufgabe wird ein Tokenizer erstellt der den Text (oder andere Inputs) vorverarbeitet, um diese dann dem Model zu übergeben. Ohne ein Tokenizer müsste man alle Vorverarbeitungsschritte selber durchführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f29eafa-42c2-4697-9418-2ec30b211f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2437, 389, 345, 1804, 30], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de847b9a-8579-419c-8244-4bcb8acde61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The weather is sunny today, so I will need \"\n",
    "\n",
    "# return_tensors: Rückgabetyp, hier: PyTorch. \n",
    "encoded_input  = tokenizer(prompt, return_tensors=\"pt\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa09057f-7918-4342-939f-20231e68696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids      = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf819d4-01e7-4281-b0eb-79025dfc3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(\n",
    "    input_ids,  # Startpunkt für die Generierung.\n",
    "    attention_mask=attention_mask,  # Ignoriere padding Tokens.  \n",
    "    do_sample=True,   # Erlaubt Sampling:True: erlaubt kreative outputs. \n",
    "    temperature=0.5,  # Wie \"kreativ\" das Model sein soll => Randomness \n",
    "    max_length=25,    # Output Länge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3d088f-f699-4d6e-96be-1eb6e3a6b956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220,  1849,    64,  1310,  1643,   286,  6290,   284,   651,   284,\n",
       "           262,  5093,   286,   262,  1748]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e691d9db-f702-43d1-b566-86db64ecadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is sunny today, so I will need  a little bit of rain to get to the north of the city\n"
     ]
    }
   ],
   "source": [
    "# batch_decode: Token IDs zurück zu Wörtern. \n",
    "# - Weitere Wörter werden generiert.\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7709-f35b-4703-aa5e-31fff9b7e698",
   "metadata": {},
   "source": [
    "Das war ein erstes einfaches Beispiel wie man ein Model nutzt. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42184e6-e039-4706-9f4f-48ce30e88aa2",
   "metadata": {},
   "source": [
    "<h2>Promt Templates und Chains</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e356c-bfb6-446c-8935-bdc43bc2758c",
   "metadata": {},
   "source": [
    "Statt immer wieder den String zu ändern, können Templates verwendet werden, um Änderungen direkt im Text vorzunehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395a90c5-3de0-4716-8882-6370ccf9bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813c75ca-1f2f-43d7-9e15-266e9924b93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today the weather is sunny, so I will... '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Template\n",
    "my_template = PromptTemplate(\n",
    "    input_variables= ['replace'],\n",
    "    template=        \"Today the weather is {replace}, so I will... \"\n",
    ")\n",
    "\n",
    "# Teste Template\n",
    "my_template.format(replace=\"sunny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd899edd-401e-41cc-9ec9-c95fd1eda4af",
   "metadata": {},
   "source": [
    "So können verschiedenen Elemente direkt im Text ersetzt werden. Wie der Name schon sagt, wird ein Template erstellt, wo bestimmte Lücken gefüllt werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3a8c-948e-4349-99b6-5ac3fec8289e",
   "metadata": {},
   "source": [
    "<i>Abb2</i>: Prompt Template, fülle die Lücken.\n",
    "\n",
    "<img src=\"./data/img/2_lg.PNG\" width=400 height=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ad6497-a1f4-4b1b-a6d1-c1e1503ac692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me 5 of facts about the topic dog')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weiteres Beispiel # \n",
    "\n",
    "template = \"Tell me {x-number} of facts about the topic {topic}\"  # Text Template.\n",
    "promt_template = PromptTemplate.from_template(template)  # Erstelle Template.\n",
    "\n",
    "promt =  promt_template.invoke({\"x-number\": 5, \"topic\": \"dog\" })\n",
    "promt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0d0b3-cdd6-46e4-8f23-081cd219278c",
   "metadata": {},
   "source": [
    "Für den Anfang reicht es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0931b36-8fcd-4b58-8b48-775418226135",
   "metadata": {},
   "source": [
    "Dann gibt es noch Chains mit dem Operationen verkettet werden können. Das ist einer der Hauptbestandteile  von LangChain. Verschiedene Aufgaben können durch eine Verkettung zusammengefügt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca78c6-ec7b-4794-8af9-c97b5e871e65",
   "metadata": {},
   "source": [
    "<i>Abb3</i>: Operationen Verketten.\n",
    "\n",
    "<img src=\"./data/img/3_lg.PNG\" width=650 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc08d7-f9ea-4dee-a760-a88265437d4d",
   "metadata": {},
   "source": [
    "<i>Abb4</i>: Möglichkeiten im Überblick.\n",
    "\n",
    "<img src=\"./data/img/4_lg.PNG\" width=650 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d45dd-ceb8-4502-a4de-881ae79a1605",
   "metadata": {},
   "source": [
    "Wie erwähnt gibt es viele Modelle die sich in der Größe und den Aufgaben unterscheiden. Hier nutzen wir das Model DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6f17103d-df9c-4dbe-a990-b3c0f034aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "79a2bc22-b662-40bd-b0fd-b44075003915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "    # Parse Inputs.\n",
    "    encoded_input  = tokenizer(text.to_string(), return_tensors=\"pt\")\n",
    "    input_ids      = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    # Model Config. \n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,  \n",
    "        attention_mask=attention_mask,   \n",
    "        do_sample=True,  \n",
    "        temperature=0.7,  \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        #max_length=50,   \n",
    "        top_k=50,  \n",
    "        top_p=0.95, \n",
    "        max_new_tokens=60,   \n",
    "    )\n",
    "    # Gebe Text zurück.\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e362c6e-1370-4446-93e9-8a64897385e4",
   "metadata": {},
   "source": [
    "Je Umfangreicher das Model, desto bessere Antworten kann es liefern. Es gibt auch zahlreiche Parameter die eingestellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "287ba9f9-2dce-4297-9873-9fa30e30d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is sunny, what should I do today?”\n",
      "\n",
      "\n",
      "I'm going to keep this up for a while and I will post more about it.\n",
      "Advertisements<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Template\n",
    "template = \"The weather is {weather}, what should I do today?\"  # Text Template.\n",
    "promt_template = PromptTemplate.from_template(template)  # Erstelle Template.\n",
    "# Chain\n",
    "chain = promt_template | make_prediction \n",
    "# Führe aus\n",
    "result = chain.invoke({\n",
    "    \"weather\":\"sunny\", \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a512ec6-7e31-4920-a879-ef5627252a5a",
   "metadata": {},
   "source": [
    "<h3>Erweitere Lineare Chain</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aae18-8023-40f6-a08d-3818ceb73a0a",
   "metadata": {},
   "source": [
    "Die Chain die wir haben `chain = promt_template | make_prediction ` kann z. B. durch Rannables erweitert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "372a8f25-10e5-4caa-90d9-f8bee5522bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count: 45\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# Erstelle Lambdas # \n",
    "upper_case  = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Count: { len(x.split()) }\" )\n",
    "# Eerstelle Chain\n",
    "chain = promt_template | make_prediction | upper_case | count_words  # Erweitere Chain.\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"weather\":\"sunny\", \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbafcb-2226-4353-8c0f-4ed44797578e",
   "metadata": {},
   "source": [
    "Durch Runnables kann so die Funktionalität erweitert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b5033-448f-4951-9527-4a27ec0276b1",
   "metadata": {},
   "source": [
    "<h3>Parallele Chains</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea43506-50cd-4e81-a8b2-7a48dc7f8f4a",
   "metadata": {},
   "source": [
    "<i>Abb5</i>: Aufbaubeispiel Parallele Chains.\n",
    "\n",
    "<img src=\"./data/img/5_lg.PNG\" width=400 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c21ec-1ce0-4bc6-8320-efe92b6c9c22",
   "metadata": {},
   "source": [
    "Man kann es sich so vorstellen, das man dem Model z. B. ChatGPT ein Produktname gibt und das Model soll getrennt Pros und Cons auflisten. Am Ende werden die Ergebnisse zu einer Liste vereinigt. Das erklärt ganz gut die Basisfunktionalität der parallelen Chains. \n",
    "\n",
    "Hier Nutzen wir das DialoGPT Model (small, medium, large).\n",
    "> https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/downloads/ [Letzter Zugriff: 07.08.2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf530ade-310d-4175-b217-7b55a940c3ca",
   "metadata": {},
   "source": [
    "Bei einem Chat-Model können drei Typen von Messages genutzt werden.<br>\n",
    "In diesem Kontext gibt es 3 Arten von Texten.:<br>\n",
    "- SystemMessage: Das den Kontext und das Verhalten beschreibt.\n",
    "- Human Message: Was der Mensch sagt.\n",
    "- AI Message: Was das Model ausgibt.\n",
    "\n",
    "Für den Einstieg halten wir es simpel. Es ist keine Konversation wo wir die Nachrichten festhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "c941b085-ebfc-4f5b-a53f-d2a3f8c49376",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "a6f81e75-abad-4c6c-9ab0-434dfad6c28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d26e079-3065-4d0d-ad48-a1881b5ce935",
   "metadata": {},
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "# Beispiel Vorlage.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an IT product reviewer\"),\n",
    "    HumanMessage(content=\"What are the cons of a Windows PC\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "fabc1c4e-9b00-4d15-9851-4f399e5ad991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "   \n",
    "    encoded_input  = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids      = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        do_sample=True,\n",
    "        temperature=0.7,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=200,\n",
    "    )\n",
    "    # Entferne Promt, damit nut Antwort sichtbar ist. \n",
    "    gen_text = tokenizer.decode(gen_tokens[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "01b76578-e2e6-4a22-980b-c1a8179b5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Human: Let's talk about the weather, it is very sunny here, what should I do? \\nAI:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "c1000d77-d77f-441c-ab9b-55c7b6d9ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you think the weather will be like in 2 weeks?\n"
     ]
    }
   ],
   "source": [
    "result = make_prediction(text)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b215d8a4-f947-4f5b-ae3a-038c5c7d04f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b121be3-6d27-49d4-ba05-c4bea7ec2b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eb4d86-d246-4887-b63e-259ef935a673",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
