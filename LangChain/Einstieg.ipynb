{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966a7f4f-b0e6-4d49-933f-36ff5258d988",
   "metadata": {},
   "source": [
    "<h1>LangChain Beispiel</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b556eb7-28ee-4a8c-a1f9-69959c1234f6",
   "metadata": {},
   "source": [
    "Wieso sollte man LangChain nutzen und nicht einfach ChatGPT? <br>\n",
    "Oft will man ein eigenes LLM Model nutzen das mit den eigenen bestehenden Daten trainiert wurde. Das hat den Vorteil das keine sensitiven Daten nach außen dringen zu Webseiten, die ein Model bereitstellen und potenziell diese Daten die geschickt wurden speichert. Mit einem eigenen Model bleibt das Model Lokal auf dem PC oder im Unternehmensnetzwerk. Zudem haben Modelle die ChatGPT keinen direkten Zugriff auf unsere Daten.\n",
    "\n",
    "Open-Source Modelle können auch einfach importiert und gespeichert werden, damit fällt das Haupttraining des Netzes weg (ggf. Fine-Tuning). Mit LangChain können solche Applikationen mit Modellen erstellt werden. Das Framework bietet verschiedenste Tools für den Umgang mit solchen Modellen.\n",
    "\n",
    "Als Einstieg wollen wir ein Open-Source Modell nutzen und einfache Textabfragen abwickeln. Weitere Beispiele Folgen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa157755-c197-4c87-8988-4890abcb5adf",
   "metadata": {},
   "source": [
    "<i>Abb1</i>: ChatGPT nutzung. Kommunikation über Internet.\n",
    "\n",
    "<img src=\"./data/img/1_lg.PNG\" width=700 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b191e81-9056-4345-a739-8b62418f4a6e",
   "metadata": {},
   "source": [
    "*Um ChatGPT zu nutzen, muss ein OpenAI API Key bereitgestellt werden. Siehe OpenAI API <br>\n",
    "*Kosten können anfallen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15aa1f-5477-4427-b753-56682ff633cf",
   "metadata": {},
   "source": [
    "Mehr über LangChain:\n",
    "\n",
    "> https://python.langchain.com/v0.2/docs/introduction/ [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "HuggingFace Modelle und Beispiele: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f99f-ea55-4112-93a6-f8667dcadfc1",
   "metadata": {},
   "source": [
    "Es gibt eine große Anzahl von Modellen, die wir nutzen können. Als Einstieg nutzen wir das GPT2 Model.<br>\n",
    "Auf der Webseite von HuggingFace wo die Modelle aufgelistet sind, befinden sich für jedes Model Beschreibungen und Beispiele.\n",
    "\n",
    "Es sollte genug Speicher für ein Model bereitgestellt werden. \n",
    "\n",
    "GPT2: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "Dieses Model wurde mit den Daten aus 8 Millionen Webseiten trainiert. <u>Das Ziel</u>: die nächsten Wörter vorhersagen.\n",
    "\n",
    "Jedes Model kann andere Ziele haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7242cca5-e684-465a-8a5c-223d906e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28fccbf0-1397-4fc2-b14e-c3bb17da4224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Model.\n",
    "model       = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Model, welches wir nutzen wollen. \n",
    "tokenizer   = AutoTokenizer.from_pretrained(\"gpt2\")  # Tokenizer des Models => Bereite Eingabe vor. \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98b80b-af3b-4045-97bc-32b5280e6c9b",
   "metadata": {},
   "source": [
    "Jedes Model ist anders und braucht einen anderen Input. Je nach Model und Aufgabe wird ein Tokenizer erstellt der den Text (oder andere Inputs) vorverarbeitet, um diese dann dem Model zu übergeben. Ohne ein Tokenizer müsste man alle Vorverarbeitungsschritte selber durchführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f29eafa-42c2-4697-9418-2ec30b211f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2437, 389, 345, 1804, 30], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de847b9a-8579-419c-8244-4bcb8acde61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The weather is sunny today, so I will need \"\n",
    "\n",
    "# return_tensors: Rückgabetyp, hier: PyTorch. \n",
    "encoded_input  = tokenizer(prompt, return_tensors=\"pt\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa09057f-7918-4342-939f-20231e68696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids      = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bf819d4-01e7-4281-b0eb-79025dfc3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(\n",
    "    input_ids,  # Startpunkt für die Generierung.\n",
    "    attention_mask=attention_mask,  # Ignoriere padding Tokens.  \n",
    "    do_sample=True,   # Erlaubt Sampling:True: erlaubt kreative outputs. \n",
    "    temperature=0.5,  # Wie \"kreativ\" das Model sein soll => Randomness \n",
    "    max_length=25,    # Output Länge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d3d088f-f699-4d6e-96be-1eb6e3a6b956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220,  3711,  8887,   290,   220,  3711,  6891,   284,   651,   832,\n",
       "           428,    13,   314,   716,  1016]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e691d9db-f702-43d1-b566-86db64ecadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is sunny today, so I will need iced tea and iced coffee to get through this. I am going\n"
     ]
    }
   ],
   "source": [
    "# batch_decode: Token IDs zurück zu Wörtern. \n",
    "# - Weitere Wörter werden generiert.\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7709-f35b-4703-aa5e-31fff9b7e698",
   "metadata": {},
   "source": [
    "Das war ein erstes einfaches Beispiel wie man ein Model nutzt. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42184e6-e039-4706-9f4f-48ce30e88aa2",
   "metadata": {},
   "source": [
    "<h2>Promt Templates und Chains</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e356c-bfb6-446c-8935-bdc43bc2758c",
   "metadata": {},
   "source": [
    "Statt immer wieder den String zu ändern, können Templates verwendet werden, um Änderungen direkt im Text vorzunehmen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "395a90c5-3de0-4716-8882-6370ccf9bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "813c75ca-1f2f-43d7-9e15-266e9924b93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today the weather is sunny, so I will... '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Template\n",
    "my_template = PromptTemplate(\n",
    "    input_variables= ['replace'],\n",
    "    template=        \"Today the weather is {replace}, so I will... \"\n",
    ")\n",
    "\n",
    "# Teste Template\n",
    "my_template.format(replace=\"sunny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd899edd-401e-41cc-9ec9-c95fd1eda4af",
   "metadata": {},
   "source": [
    "So können verschiedenen Elemente direkt im Text ersetzt werden. Wie der Name schon sagt, wird ein Template erstellt, wo bestimmte Lücken gefüllt werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3a8c-948e-4349-99b6-5ac3fec8289e",
   "metadata": {},
   "source": [
    "<i>Abb2</i>: Prompt Template, fülle die Lücken.\n",
    "\n",
    "<img src=\"./data/img/2_lg.PNG\" width=400 height=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35ad6497-a1f4-4b1b-a6d1-c1e1503ac692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me 5 of facts about the topic dog')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weiteres Beispiel # \n",
    "\n",
    "template = \"Tell me {x-number} of facts about the topic {topic}\"  # Text Template.\n",
    "promt_template = PromptTemplate.from_template(template)  # Erstelle Template.\n",
    "\n",
    "promt =  promt_template.invoke({\"x-number\": 5, \"topic\": \"dog\" })\n",
    "promt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0d0b3-cdd6-46e4-8f23-081cd219278c",
   "metadata": {},
   "source": [
    "Für den Anfang reicht es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0931b36-8fcd-4b58-8b48-775418226135",
   "metadata": {},
   "source": [
    "Dann gibt es noch Chains mit dem Operationen verkettet werden können. Das ist einer der Hauptbestandteile  von LangChain. Verschiedene Aufgaben können durch eine Verkettung zusammengefügt werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ca78c6-ec7b-4794-8af9-c97b5e871e65",
   "metadata": {},
   "source": [
    "<i>Abb3</i>: Operationen Verketten.\n",
    "\n",
    "<img src=\"./data/img/3_lg.PNG\" width=650 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc08d7-f9ea-4dee-a760-a88265437d4d",
   "metadata": {},
   "source": [
    "<i>Abb4</i>: Möglichkeiten im Überblick.\n",
    "\n",
    "<img src=\"./data/img/4_lg.PNG\" width=650 height=450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546d45dd-ceb8-4502-a4de-881ae79a1605",
   "metadata": {},
   "source": [
    "Wie erwähnt gibt es viele Modelle die sich in der Größe und den Aufgaben unterscheiden. Hier nutzen wir das Model DistilGPT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f17103d-df9c-4dbe-a990-b3c0f034aded",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilgpt2\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name)\n",
    "model      = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "79a2bc22-b662-40bd-b0fd-b44075003915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "    # Parse Inputs.\n",
    "    encoded_input  = tokenizer(text.to_string(), return_tensors=\"pt\")\n",
    "    input_ids      = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "    # Model Config. \n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,  \n",
    "        attention_mask=attention_mask,   \n",
    "        do_sample=True,  \n",
    "        temperature=0.7,  \n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        #max_length=50,   \n",
    "        top_k=50,  \n",
    "        top_p=0.95, \n",
    "        max_new_tokens=60,   \n",
    "    )\n",
    "    # Gebe Text zurück.\n",
    "    gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "    return gen_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e362c6e-1370-4446-93e9-8a64897385e4",
   "metadata": {},
   "source": [
    "Je Umfangreicher das Model, desto bessere Antworten kann es liefern. Es gibt auch zahlreiche Parameter die eingestellt werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "287ba9f9-2dce-4297-9873-9fa30e30d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is sunny, what should I do today?”\n",
      "\n",
      "\n",
      "I was in my early 20s and was looking forward to my first day in a row with my wife and our 4-year-old daughter.\n",
      "It was very cold and humid, and there was no food or the water. I was in the shower, so I\n"
     ]
    }
   ],
   "source": [
    "# Template\n",
    "template = \"The weather is {weather}, what should I do today?\"  # Text Template.\n",
    "promt_template = PromptTemplate.from_template(template)  # Erstelle Template.\n",
    "# Chain\n",
    "chain = promt_template | make_prediction \n",
    "# Führe aus\n",
    "result = chain.invoke({\n",
    "    \"weather\":\"sunny\", \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a512ec6-7e31-4920-a879-ef5627252a5a",
   "metadata": {},
   "source": [
    "<h3>Erweitere Lineare Chain</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056aae18-8023-40f6-a08d-3818ceb73a0a",
   "metadata": {},
   "source": [
    "Die Chain die wir haben `chain = promt_template | make_prediction ` kann z. B. durch Rannables erweitert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "372a8f25-10e5-4caa-90d9-f8bee5522bac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Input to PromptTemplate is missing variables {'animal', 'like'}.  Expected: ['animal', 'like'] Received: ['weather']\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[222], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Eerstelle Chain\u001b[39;00m\n\u001b[0;32m      7\u001b[0m chain \u001b[38;5;241m=\u001b[39m promt_template \u001b[38;5;241m|\u001b[39m make_prediction \u001b[38;5;241m|\u001b[39m upper_case \u001b[38;5;241m|\u001b[39m count_words  \u001b[38;5;66;03m# Erweitere Chain.\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m result \u001b[38;5;241m=\u001b[39m chain\u001b[38;5;241m.\u001b[39minvoke({\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweather\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msunny\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     11\u001b[0m })\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(result)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\runnables\\base.py:2873\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   2869\u001b[0m config \u001b[38;5;241m=\u001b[39m patch_config(\n\u001b[0;32m   2870\u001b[0m     config, callbacks\u001b[38;5;241m=\u001b[39mrun_manager\u001b[38;5;241m.\u001b[39mget_child(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseq:step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2871\u001b[0m )\n\u001b[0;32m   2872\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2873\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2874\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2875\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\prompts\\base.py:179\u001b[0m, in \u001b[0;36mBasePromptTemplate.invoke\u001b[1;34m(self, input, config)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags:\n\u001b[0;32m    178\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtags\n\u001b[1;32m--> 179\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_prompt_with_error_handling,\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m    182\u001b[0m     config,\n\u001b[0;32m    183\u001b[0m     run_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    184\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\runnables\\base.py:1784\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[1;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[0;32m   1780\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[0;32m   1781\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[0;32m   1782\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m   1783\u001b[0m         Output,\n\u001b[1;32m-> 1784\u001b[0m         context\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m   1785\u001b[0m             call_func_with_variable_args,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1786\u001b[0m             func,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m             \u001b[38;5;28minput\u001b[39m,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m             config,\n\u001b[0;32m   1789\u001b[0m             run_manager,\n\u001b[0;32m   1790\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   1791\u001b[0m         ),\n\u001b[0;32m   1792\u001b[0m     )\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1794\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\runnables\\config.py:428\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[1;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[0;32m    427\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[1;32m--> 428\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\prompts\\base.py:153\u001b[0m, in \u001b[0;36mBasePromptTemplate._format_prompt_with_error_handling\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_format_prompt_with_error_handling\u001b[39m(\u001b[38;5;28mself\u001b[39m, inner_input: Dict) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptValue:\n\u001b[1;32m--> 153\u001b[0m     _inner_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_input(inner_input)\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_prompt(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_inner_input)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\pytorch_\\Lib\\site-packages\\langchain_core\\prompts\\base.py:145\u001b[0m, in \u001b[0;36mBasePromptTemplate._validate_input\u001b[1;34m(self, inner_input)\u001b[0m\n\u001b[0;32m    143\u001b[0m missing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables)\u001b[38;5;241m.\u001b[39mdifference(inner_input)\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is missing variables \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_variables\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inner_input\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    149\u001b[0m     )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_input\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Input to PromptTemplate is missing variables {'animal', 'like'}.  Expected: ['animal', 'like'] Received: ['weather']\""
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "\n",
    "# Erstelle Lambdas # \n",
    "upper_case  = RunnableLambda(lambda x: x.upper())\n",
    "count_words = RunnableLambda(lambda x: f\"Count: { len(x.split()) }\" )\n",
    "# Eerstelle Chain\n",
    "chain = promt_template | make_prediction | upper_case | count_words  # Erweitere Chain.\n",
    "\n",
    "result = chain.invoke({\n",
    "    \"weather\":\"sunny\", \n",
    "})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddbafcb-2226-4353-8c0f-4ed44797578e",
   "metadata": {},
   "source": [
    "Durch Runnables kann so die Funktionalität erweitert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6b5033-448f-4951-9527-4a27ec0276b1",
   "metadata": {},
   "source": [
    "<h3>Parallele Chains</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea43506-50cd-4e81-a8b2-7a48dc7f8f4a",
   "metadata": {},
   "source": [
    "<i>Abb5</i>: Aufbaubeispiel Parallele Chains.\n",
    "\n",
    "<img src=\"./data/img/5_lg.PNG\" width=400 height=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c21ec-1ce0-4bc6-8320-efe92b6c9c22",
   "metadata": {},
   "source": [
    "Man kann es sich so vorstellen, das man dem Model z. B. ChatGPT ein Produktname gibt und das Model soll getrennt Pros und Cons auflisten. Am Ende werden die Ergebnisse zu einer Liste vereinigt. Das erklärt ganz gut die Basisfunktionalität der parallelen Chains. \n",
    "\n",
    "Hier Nutzen wir das DialoGPT Model (small, medium, large).\n",
    "> https://www.microsoft.com/en-us/research/project/large-scale-pretraining-for-response-generation/downloads/ [Letzter Zugriff: 07.08.2024]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf530ade-310d-4175-b217-7b55a940c3ca",
   "metadata": {},
   "source": [
    "Bei einem Chat-Model können drei Typen von Messages genutzt werden.<br>\n",
    "In diesem Kontext gibt es 3 Arten von Texten.:<br>\n",
    "- SystemMessage: Das den Kontext und das Verhalten beschreibt.\n",
    "- Human Message: Was der Mensch sagt.\n",
    "- AI Message: Was das Model ausgibt.\n",
    "\n",
    "Für den Einstieg halten wir es simpel. Es ist keine Konversation wo wir die Nachrichten festhalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c941b085-ebfc-4f5b-a53f-d2a3f8c49376",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a6f81e75-abad-4c6c-9ab0-434dfad6c28c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_name = \"microsoft/DialoGPT-medium\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d26e079-3065-4d0d-ad48-a1881b5ce935",
   "metadata": {},
   "source": [
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "# Beispiel Vorlage.\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an IT product reviewer\"),\n",
    "    HumanMessage(content=\"What are the cons of a Windows PC\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fabc1c4e-9b00-4d15-9851-4f399e5ad991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(text):\n",
    "   \n",
    "    encoded_input  = tokenizer(text, return_tensors=\"pt\")\n",
    "    input_ids      = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        do_sample=True,\n",
    "        temperature=0.75,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "    )\n",
    "    # Entferne Promt, damit nut Antwort sichtbar ist. \n",
    "    gen_text = tokenizer.decode(gen_tokens[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return gen_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "01b76578-e2e6-4a22-980b-c1a8179b5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Human: Let's talk about the weather, it is very sunny here, what should I do? \\nAI:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "c1000d77-d77f-441c-ab9b-55c7b6d9ffcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm going to use a lot of sun, I will get better at snowboarding.\n"
     ]
    }
   ],
   "source": [
    "result = make_prediction(text)\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31374a0b-fd2c-4f03-940e-7272e9cdd417",
   "metadata": {},
   "source": [
    "Für ein einfaches Beispiel werden zwei verschiedene Anfragen gestellt und später zusammengefügt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "3150589b-6306-4e57-8d1b-f55ea7b4b6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No, humans are not animals. Humans can be animals.\n"
     ]
    }
   ],
   "source": [
    "# Erstelle Basistemplate # \n",
    "template = \"Human: You are an Animal expert, is it true that {animal} like {like}? \\nAI:\" \n",
    "promt_template = PromptTemplate.from_template(template)  \n",
    "\n",
    "result = make_prediction(promt_template.invoke({\"animal\": \"dogs\", \"like\": \"milk\"}).to_string())\n",
    "print(result.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "1fa93318-4704-491f-9fb6-30969e4cd991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction_2(text):\n",
    "   \n",
    "    encoded_input  = tokenizer(text.to_string(), return_tensors=\"pt\")\n",
    "    input_ids      = encoded_input['input_ids']\n",
    "    attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "    gen_tokens = model.generate(\n",
    "        input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        do_sample=True,\n",
    "        temperature=0.75,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=400,\n",
    "    )\n",
    "    # Entferne Promt, damit nut Antwort sichtbar ist. \n",
    "    gen_text = tokenizer.decode(gen_tokens[:, input_ids.shape[-1]:][0], skip_special_tokens=True)\n",
    "    return gen_text.strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18388798-973a-406a-946e-f088adde13a6",
   "metadata": {},
   "source": [
    "Jetzt könnten wir diesen Text mit einem anderen größeren Model verarbeiten.<br>\n",
    "- [Issue]: Gerade gibt es Probleme mit dem Anwenden größerer Modelle. Hier wird ein anderer Ansatz genutzt. <br>\n",
    "  Statt ein Model zu nutzen werden Funktionen verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "de771b62-201a-42a5-8968-6ed5be128152",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erstelle Funktionen\n",
    "def count_words(text):\n",
    "    return len(text.split())\n",
    "def add_word_count(num):\n",
    "    return num + 500\n",
    "\n",
    "def text_transf_1(text:str):\n",
    "    return text.upper()\n",
    "def text_transf_2(text:str):\n",
    "    return text.replace(\"a\", '-')\n",
    "    \n",
    "# Erstelle Zweige.\n",
    "# - Hier können wieder Chains erstellt werden. \n",
    "# - Weitere Templates oder Modelle. \n",
    "branch_1 = ( RunnableLambda(lambda x: count_words(x)   ))  | add_word_count\n",
    "branch_2 = ( RunnableLambda(lambda x: text_transf_1(x)  )) | text_transf_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b9f19f20-f0cc-4e3a-9fc3-165345154f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    promt_template | make_prediction_2  # Oder auch andere diverse Vorbereitungen.\n",
    "    # - Parallel:\n",
    "    | RunnableParallel(branches={\"branch_1\": branch_1, \"branch_2\": branch_2 })  # Gebe Branches an.\n",
    "    # Weitere Schritte...\n",
    "    | RunnableLambda(lambda x: f\"Result Branch_1: {x['branches']['branch_1']}\\n\" +\n",
    "                    f\"Result Branch_2: {x['branches']['branch_2']}\")\n",
    "    # Mit Keys zugreifen. \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "54c974b7-37b8-448b-a5fb-25e3dfbc39f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result Branch_1: 514\n",
      "Result Branch_2: NO, THAT'S NOT TRUE. HUMANS : MILK IS THE BEST FOOD IN THE WORLD.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"animal\": \"dogs\", \"like\": \"milk\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b8628f-7f43-453a-8671-fe9772bbb1ad",
   "metadata": {},
   "source": [
    "Durch diese parallelen Chains können beliebige viele Operationen angehangen werden die nach n-Branches zusammengefügt werden, um ein Ergebnis zu liefern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be77c726-4b95-41f2-a4ea-041062a2c81a",
   "metadata": {},
   "source": [
    "<h2>Branching</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad514f6e-71fd-49b5-9040-cc3e5165bba3",
   "metadata": {},
   "source": [
    "Hier geht es darum basierend auf einem Argument einen Branch auszuwählen. Man kann es sich wie ein If-Statement vorstellen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0717776c-94a3-4a7d-97f0-8148d0244c35",
   "metadata": {},
   "source": [
    "<i>Abb6</i>: Branching.\n",
    "\n",
    "<img src=\"./data/img/6_lg.PNG\" width=450 height=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de64ae8-6de8-4f91-85b7-96338a1fcee0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dc4177-918d-44fe-ada3-1299f71f871f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7c2a72-6cd4-4548-be26-afc6560d89a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d0be5-e538-4066-8d5d-30ac2c0dd7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
