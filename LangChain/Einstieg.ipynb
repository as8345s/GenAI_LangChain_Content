{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "966a7f4f-b0e6-4d49-933f-36ff5258d988",
   "metadata": {},
   "source": [
    "<h1>LangChain Beispiel</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b556eb7-28ee-4a8c-a1f9-69959c1234f6",
   "metadata": {},
   "source": [
    "Wieso sollte man LangChain nutzen und nicht einfach ChatGPT? <br>\n",
    "Oft will man ein eigenes LLM Model nutzen das mit den eigenen bestehenden Daten trainiert wurde. Das hat den Vorteil das keine sensitiven Daten nach außen dringen zu Webseiten, die ein Model bereitstellen und potenziell diese Daten die geschickt wurden speichert. Mit einem eigenen Model bleibt das Model Lokal auf dem PC oder im Unternehmensnetzwerk. Zudem haben Modelle die ChatGPT keinen direkten Zugriff auf unsere Daten.\n",
    "\n",
    "Open-Source Modelle können auch einfach importiert und gespeichert werden, damit fällt das Haupttraining des Netzes weg (ggf. Fine-Tuning). Mit LangChain können solche Applikationen mit Modellen erstellt werden. Das Framework bietet verschiedenste Tools für den Umgang mit solchen Modellen.\n",
    "\n",
    "Als Einstieg wollen wir ein Open-Source Modell nutzen und einfache Textabfragen abwickeln. Weitere Beispiele Folgen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa157755-c197-4c87-8988-4890abcb5adf",
   "metadata": {},
   "source": [
    "<i>Abb1</i>: ChatGPT nutzung. Kommunikation über Internet.\n",
    "\n",
    "<img src=\"./data/img/1_lg.PNG\" width=700 height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b191e81-9056-4345-a739-8b62418f4a6e",
   "metadata": {},
   "source": [
    "*Um ChatGPT zu nutzen, muss ein OpenAI API Key bereitgestellt werden. Siehe OpenAI API <br>\n",
    "*Kosten können anfallen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c15aa1f-5477-4427-b753-56682ff633cf",
   "metadata": {},
   "source": [
    "Mehr über LangChain:\n",
    "\n",
    "> https://python.langchain.com/v0.2/docs/introduction/ [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "HuggingFace Modelle und Beispiele: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f0f99f-ea55-4112-93a6-f8667dcadfc1",
   "metadata": {},
   "source": [
    "Es gibt eine große Anzahl von Modellen, die wir nutzen können. Als Einstieg nutzen wir das GPT2 Model.<br>\n",
    "Auf der Webseite von HuggingFace wo die Modelle aufgelistet sind, befinden sich für jedes Model Beschreibungen und Beispiele.\n",
    "\n",
    "Es sollte genug Speicher für ein Model bereitgestellt werden. \n",
    "\n",
    "GPT2: <br>\n",
    "> https://huggingface.co/docs/transformers/model_doc/gpt2 [Letzter Zugriff: 01.08.2024]\n",
    "\n",
    "Dieses Model wurde mit den Daten aus 8 Millionen Webseiten trainiert. <u>Das Ziel</u>: die nächsten Wörter vorhersagen.\n",
    "\n",
    "Jedes Model kann andere Ziele haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7242cca5-e684-465a-8a5c-223d906e1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports.\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28fccbf0-1397-4fc2-b14e-c3bb17da4224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2SdpaAttention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Model.\n",
    "model       = AutoModelForCausalLM.from_pretrained(\"gpt2\")  # Model, welches wir nutzen wollen. \n",
    "tokenizer   = AutoTokenizer.from_pretrained(\"gpt2\")  # Tokenizer des Models => Bereite Eingabe vor. \n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98b80b-af3b-4045-97bc-32b5280e6c9b",
   "metadata": {},
   "source": [
    "Jedes Model ist anders und braucht einen anderen Input. Je nach Model und Aufgabe wird ein Tokenizer erstellt der den Text (oder andere Inputs) vorverarbeitet, um diese dann dem Model zu übergeben. Ohne ein Tokenizer müsste man alle Vorverarbeitungsschritte selber durchführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f29eafa-42c2-4697-9418-2ec30b211f6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2437, 389, 345, 1804, 30], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"How are you doing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de847b9a-8579-419c-8244-4bcb8acde61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = \"The weather is sunny today, so I will need \"\n",
    "\n",
    "# return_tensors: Rückgabetyp, hier: PyTorch. \n",
    "encoded_input  = tokenizer(prompt, return_tensors=\"pt\")\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa09057f-7918-4342-939f-20231e68696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids      = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bf819d4-01e7-4281-b0eb-79025dfc3b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "gen_tokens = model.generate(\n",
    "    input_ids,  # Startpunkt für die Generierung.\n",
    "    attention_mask=attention_mask,  # Ignoriere padding Tokens.  \n",
    "    do_sample=True,   # Erlaubt Sampling:True: erlaubt kreative outputs. \n",
    "    temperature=0.5,  # Wie \"kreativ\" das Model sein soll => Randomness \n",
    "    max_length=25,    # Output Länge\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d3d088f-f699-4d6e-96be-1eb6e3a6b956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  6193,   318, 27737,  1909,    11,   523,   314,   481,   761,\n",
       "           220,  1849,    64,  1310,  1643,   286,  6290,   284,   651,   284,\n",
       "           262,  5093,   286,   262,  1748]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e691d9db-f702-43d1-b566-86db64ecadd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weather is sunny today, so I will need  a little bit of rain to get to the north of the city\n"
     ]
    }
   ],
   "source": [
    "# batch_decode: Token IDs zurück zu Wörtern. \n",
    "# - Weitere Wörter werden generiert.\n",
    "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c7709-f35b-4703-aa5e-31fff9b7e698",
   "metadata": {},
   "source": [
    "Das war ein erstes einfaches Beispiel wie man ein Model nutzt. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42184e6-e039-4706-9f4f-48ce30e88aa2",
   "metadata": {},
   "source": [
    "<h2>Promt Templates und Chains</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561e356c-bfb6-446c-8935-bdc43bc2758c",
   "metadata": {},
   "source": [
    "Statt immer wieder den String zu ändern, können Templates verwendet werden, um Änderungen direkt im Text vorzunehmen.\n",
    "\n",
    "Hier nutzen wir dasselbe Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "395a90c5-3de0-4716-8882-6370ccf9bc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "813c75ca-1f2f-43d7-9e15-266e9924b93e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Today the weather is sunny, so I will... '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Erstelle Template\n",
    "my_template = PromptTemplate(\n",
    "    input_variables= ['replace'],\n",
    "    template=        \"Today the weather is {replace}, so I will... \"\n",
    ")\n",
    "\n",
    "# Teste Template\n",
    "my_template.format(replace=\"sunny\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd899edd-401e-41cc-9ec9-c95fd1eda4af",
   "metadata": {},
   "source": [
    "So können verschiedenen Elemente direkt im Text ersetzt werden. Wie der Name schon sagt, wird ein Template erstellt, wo bestimmte Lücken gefüllt werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d3a8c-948e-4349-99b6-5ac3fec8289e",
   "metadata": {},
   "source": [
    "<i>Abb2</i>: Prompt Template, fülle die Lücken.\n",
    "\n",
    "<img src=\"./data/img/2_lg.PNG\" width=400 height=200>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35ad6497-a1f4-4b1b-a6d1-c1e1503ac692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me 5 of facts about the topic dog')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weiteres Beispiel # \n",
    "\n",
    "template = \"Tell me {x-number} of facts about the topic {topic}\"  # Text Template.\n",
    "promt_template = PromptTemplate.from_template(template)  # Erstelle Template.\n",
    "\n",
    "promt =  promt_template.invoke({\"x-number\": 5, \"topic\": \"dog\" })\n",
    "promt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf0d0b3-cdd6-46e4-8f23-081cd219278c",
   "metadata": {},
   "source": [
    "Für den Anfang reicht es."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0931b36-8fcd-4b58-8b48-775418226135",
   "metadata": {},
   "source": [
    "Dann gibt es noch Chains mit dem Operationen verkettet werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792a0fac-3639-4c54-8359-145ecf1e044b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d1bde-5afa-4cb5-8107-b2f795e03cfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda5c4bb-2fa0-49b5-984a-637fe57145b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b82c0f7b-dc6f-40f7-adb6-a7157dc086b4",
   "metadata": {},
   "source": [
    "<i>Abb1:</i> [coming soon]\n",
    "\n",
    "<img src=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b694f-1eac-46aa-b0fb-41476f865907",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f615234-0b17-4d80-bf34-21010d3f7be7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276cd41-e744-4541-bd22-428a31e56ed6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
